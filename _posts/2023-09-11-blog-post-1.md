---
title: 'AutoEncoders: minimal working examples'
date: 2023-09-11
permalink: /posts/2023/09/autoencoders_minimum_working_examples/
# tags:
#   - cool posts
#   - category1
#   - category2
---

AutoEncoders allow extracting a low-dimensional embedding of an input image and reconstructing the original image from the low-dimensional embedding. While Principal Component Analysis also allows extracting such features, autoencoders are capable of learning more efficient representations due to the non-linearities of a neural network.

Sampling from the low-dimensional embedding space (also called the _latent space_) can help reconstruct new images that the network was not originally trained on, thus making autoencoders a member of the family of [generative models](https://openai.com/research/generative-models). There are some caveats about sampling from the latent space of autoencoders due to lack of "continuity", which is where Variational Auto Encoders (VAE) come in. This article however is limited to AutoEncoders.

The origin of the idea of autoencoders can be difficult to track down, see [What is the origin of the autoencoder neural networks?](https://stats.stackexchange.com/questions/238381/what-is-the-origin-of-the-autoencoder-neural-networks).

## Architecture
An AutoEncoder (henceforth referred to as AE) has an encoder-decoder architecture, with the decoder "undoing" the operations of the encoder on the original image. Squashed between the encoder and the decoder is the latent space (referred to as `Code` in the picture below), which is what learns the low-dimensional embeddings of a training dataset.

<figure>
  <img src="/images/autoencoders/Autoencoder_schema.png" alt="Autoencoder_schema">
  <figcaption>Picture from Wikipedia: https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png</figcaption>
</figure>

The network is generally trained with a Mean-Squared-Error loss function between the image reconstructed by the decoder and the input image.

## PyTorch implementation
Familiarity with PyTorch is assumed. The minimal working examples presented below use the very easily accessible MNIST dataset.

We begin with the usual imports:
```
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torch.utils.data import DataLoader

from torchvision import datasets, transforms

from matplotlib import pyplot as plt
```

Pick the device:
```
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"device: {device}")
```

Fetch the MNIST dataset, which comes with `torchvision`'s `datasets`:
```
dataset = datasets.MNIST(root="dataset/", train=True, transform=transforms.ToTensor(), download=True)
train_loader = DataLoader(dataset=dataset, batch_size=64, shuffle=True)

print(f"Size of an image: {dataset[0][0].shape}")   # torch.Size([1, 28, 28])
```

Set the hyper-parameters:
```
learning_rate = 1e-3
weight_decay = 1e-5
epochs = 20
```

Let's start out by implementing a very simple AE comprising of only Fully Connected layers. Then we'll get on to Convolutional Layers. Create a boolean variable `use_conv_autoencoder` to be able to easily pick between the two.

### Fully Connected Layers only
Ensure `use_conv_autoencoder` is False.

The class `AutoencoderLinear` below comprises of an `encoder` and its mirror-image `decoder`, which are both called in the `forward()` method.

Note that `self._img_size = 28` is specific to the MNIST dataset only.
```
class AutoencoderLinear(nn.Module):
    def __init__(self):
        super().__init__()

        self._img_size = 28 * 28

        self.encoder = nn.Sequential(
            nn.Linear(self._img_size, 128),  # N, 784 -> N, 128
            nn.ReLU(),
            nn.Linear(128, 64),              # N, 128 -> N, 64
            nn.ReLU(),
            nn.Linear(64, 12),               # N, 64 -> N, 12
            nn.ReLU(),
            nn.Linear(12, 10),               # N, 12 -> N, 3
        )

        self.decoder = nn.Sequential(
            nn.Linear(10, 12),               # N, 3 -> N, 12
            nn.ReLU(),
            nn.Linear(12, 64),               # N, 12 -> N, 64
            nn.ReLU(),
            nn.Linear(64, 128),              # N, 64 -> N, 128
            nn.ReLU(),
            nn.Linear(128, self._img_size),  # N, 128 -> N, 784
            nn.Sigmoid()
        )

    def forward(self, x):
        x_enc = self.encoder(x)
        x_dec = self.decoder(x_enc)
        return x_dec
```

Let's instantiate the model and define the loss function and the optimizer:
```
model = AutoencoderLinear()
loss_fn = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
```

Finally, let's kick off training, with the very standard training loop. We store intermediate progress in `training_progress` to be able to later visualize how the model gradually learns and improves.
```
print("Model initialized, starting training ..")
training_progress = []
for i in range(epochs):
    for (img, _) in train_loader:
        if not use_conv_autoencoder:    # use_conv_autoencoder should be False
            img = img.reshape(-1, 28 * 28)  # 28, 28 -> 1, 784
        img_reconstructed = model(img)

        loss = loss_fn(img_reconstructed, img)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch {i+1}/{epochs}: loss = {loss.item():.6f}")
    training_progress.append((i, img, img_reconstructed, loss.item()))

print("Training completed")
```

As the training progresses, you should see the `loss` gradually go down, which is indicative of the model learning something useful. Once training completes, let's use `training_progress` to visualize how the model gradually learned:
```
print("Visualizing..")
for epoch in [0, 4, 9, 14, 19]: # range(0, epochs, 5):
  print(f"Epoch {epoch + 1}")
  plt.figure(figsize=(9, 2))
  plt.gray()
  imgs = training_progress[epoch][1].detach().numpy()
  recon = training_progress[epoch][2].detach().numpy()
  loss = training_progress[epoch][3]
  plt.suptitle(f"Epoch {epoch + 1} / {epochs}: loss = {loss:.6f}")
  for i, item in enumerate(imgs):
    if i >= 9: break
    plt.subplot(2, 9, i+1)
    plt.xticks([])
    plt.yticks([])
    if not use_conv_autoencoder:    # use_conv_autoencoder should be False
      item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear
    # item: 1, 28, 28
    plt.imshow(item[0])

  for i, item in enumerate(recon):
    if i >= 9: break
    plt.subplot(2, 9, 9+i+1) # row_length + i + 1
    plt.xticks([])
    plt.yticks([])
    if not use_conv_autoencoder:    # use_conv_autoencoder should be False
      item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear
    # item: 1, 28, 28
    plt.imshow(item[0])
```
![epoch_1](/images/autoencoders/Linear/AutoEncoderLinear_epoch_0.png)
![epoch_5](/images/autoencoders/Linear/AutoEncoderLinear_epoch_4.png)
![epoch_10](/images/autoencoders/Linear/AutoEncoderLinear_epoch_9.png)
![epoch_15](/images/autoencoders/Linear/AutoEncoderLinear_epoch_14.png)
![epoch_19](/images/autoencoders/Linear/AutoEncoderLinear_epoch_19.png)

As the network learned to produce better reconstructions, the loss dropped from `0.048625` to `0.025885` by the 20th epoch. The images are still quite blurry but improvement is noticeable.

Play around with the hyperparameters to see if better reconstructions can be achieved with this network.

## References
The following links were used as source materials for this article:
- [Autoencoder In PyTorch - Theory & Implementation](https://youtu.be/zp8clK9yCro)
- [Transpose Convolutions and Autoencoders](https://www.cs.toronto.edu/~lczhang/360/lec/w05/autoencoder.html)
- [A Simple AutoEncoder and Latent Space Visualization with PyTorch](https://medium.com/@outerrencedl/a-simple-autoencoder-and-latent-space-visualization-with-pytorch-568e4cd2112a)

