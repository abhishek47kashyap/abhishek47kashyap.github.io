---
title: 'AutoEncoders: minimal working examples'
date: 2023-09-11
permalink: /posts/2023/09/autoencoders_minimum_working_examples/
# tags:
#   - cool posts
#   - category1
#   - category2
---

AutoEncoders allow extracting a low-dimensional embedding of an input image and reconstructing the original image from the low-dimensional embedding. While Principal Component Analysis also allows extracting such features, autoencoders are capable of learning more efficient representations due to the non-linearities of a neural network.

Sampling from the low-dimensional embedding space (also called the _latent space_) can help reconstruct new images that the network was not originally trained on, thus making autoencoders a member of the family of [generative models](https://openai.com/research/generative-models). There are some caveats about sampling from the latent space of autoencoders due to lack of "continuity", which is where Variational Auto Encoders (VAE) come in. This article however is limited to AutoEncoders.

## Architecture
An AutoEncoder (henceforth referred to as AE) has an encoder-decoder architecture, with the decoder "undoing" the operations of the encoder on the original image. Squashed between the encoder and the decoder is the latent space (referred to as `Code` in the picture below), which is what learns the low-dimensional embeddings of a training dataset.

<figure>
  <img src="/images/autoencoders/Autoencoder_schema.png" alt="Autoencoder_schema">
  <figcaption>Picture from Wikipedia: https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png</figcaption>
</figure>

The network is generally trained with a Mean-Squared-Error loss function between the image reconstructed by the decoder and the input image.

## PyTorch implementation
Familiarity with PyTorch is assumed. The minimal working examples presented below use the very easily accessible MNIST dataset.

Use of a notebook (like Google Colab) is recommended.

We begin with the usual imports:
```
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torch.utils.data import DataLoader

from torchvision import datasets, transforms

from matplotlib import pyplot as plt

from random import choice
```

Pick the device:
```
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"device: {device}")
```

Fetch the MNIST dataset, which comes with `torchvision`'s `datasets`:
```
dataset = datasets.MNIST(root="dataset/", train=True, transform=transforms.ToTensor(), download=True)
train_loader = DataLoader(dataset=dataset, batch_size=64, shuffle=True)

print(f"Size of an image: {dataset[0][0].shape}")   # torch.Size([1, 28, 28])
```

Set the hyper-parameters:
```
learning_rate = 1e-3
weight_decay = 1e-5
epochs = 20
```

Let's start out by implementing a very simple AE comprising of only Fully Connected layers. Then we'll get on to Convolutional Layers. Create a boolean variable `use_conv_autoencoder` to be able to easily switch between the two.

### Fully Connected Layers only
Ensure `use_conv_autoencoder` is False.

The class `AutoencoderLinear` below comprises of an `encoder` and its mirror-image `decoder`, which are both called in the `forward()` method.

Note that `self._img_size = 28` is specific to the MNIST dataset only.
```
class AutoencoderLinear(nn.Module):
    def __init__(self):
        super().__init__()

        self._img_size = 28 * 28

        self.encoder = nn.Sequential(
            nn.Linear(self._img_size, 128),  # N, 784 -> N, 128
            nn.ReLU(),
            nn.Linear(128, 64),              # N, 128 -> N, 64
            nn.ReLU(),
            nn.Linear(64, 12),               # N, 64 -> N, 12
            nn.ReLU(),
            nn.Linear(12, 10),               # N, 12 -> N, 3
        )

        self.decoder = nn.Sequential(
            nn.Linear(10, 12),               # N, 3 -> N, 12
            nn.ReLU(),
            nn.Linear(12, 64),               # N, 12 -> N, 64
            nn.ReLU(),
            nn.Linear(64, 128),              # N, 64 -> N, 128
            nn.ReLU(),
            nn.Linear(128, self._img_size),  # N, 128 -> N, 784
            nn.Sigmoid()
        )

    def forward(self, x):
        x_enc = self.encoder(x)
        x_dec = self.decoder(x_enc)
        return x_dec
```

Let's instantiate the model and define the loss function and the optimizer:
```
model = AutoencoderLinear()
loss_fn = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
```

Finally, let's kick off training, with the very standard training loop. We store intermediate progress in `training_progress` to be able to later visualize how the model gradually learns and improves.
```
print("Model initialized, starting training ..")
training_progress = []
for i in range(epochs):
    for (img, _) in train_loader:
        if not use_conv_autoencoder:    # use_conv_autoencoder should be False
            img = img.reshape(-1, 28 * 28)  # 28, 28 -> 1, 784
        img_reconstructed = model(img)

        loss = loss_fn(img_reconstructed, img)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch {i+1}/{epochs}: loss = {loss.item():.6f}")
    training_progress.append((i, img, img_reconstructed, loss.item()))

print("Training completed")
```

As the training progresses, you should see the `loss` gradually go down, which is indicative of the model learning something useful. Once training completes, let's use `training_progress` to visualize how the model gradually learned:
```
print("Visualizing..")
for epoch in [0, 4, 9, 14, 19]: # range(0, epochs, 5):
  print(f"Epoch {epoch + 1}")
  plt.figure(figsize=(9, 2))
  plt.gray()
  imgs = training_progress[epoch][1].detach().numpy()
  recon = training_progress[epoch][2].detach().numpy()
  loss = training_progress[epoch][3]
  plt.suptitle(f"Epoch {epoch + 1} / {epochs}: loss = {loss:.6f}")
  for i, item in enumerate(imgs):
    if i >= 9: break
    plt.subplot(2, 9, i+1)
    plt.xticks([])
    plt.yticks([])
    if not use_conv_autoencoder:    # use_conv_autoencoder should be False
      item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear
    # item: 1, 28, 28
    plt.imshow(item[0])

  for i, item in enumerate(recon):
    if i >= 9: break
    plt.subplot(2, 9, 9+i+1) # row_length + i + 1
    plt.xticks([])
    plt.yticks([])
    if not use_conv_autoencoder:    # use_conv_autoencoder should be False
      item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear
    # item: 1, 28, 28
    plt.imshow(item[0])
```
![epoch_1](/images/autoencoders/Linear/AutoEncoderLinear_epoch_0.png)
![epoch_5](/images/autoencoders/Linear/AutoEncoderLinear_epoch_4.png)
![epoch_10](/images/autoencoders/Linear/AutoEncoderLinear_epoch_9.png)
![epoch_15](/images/autoencoders/Linear/AutoEncoderLinear_epoch_14.png)
![epoch_19](/images/autoencoders/Linear/AutoEncoderLinear_epoch_19.png)

As the network learned to produce better reconstructions, the loss dropped from `0.048625` to `0.025885` by the 20th epoch. The reconstructions are still quite blurry but can be deemed legible.

Play around with the hyperparameters to see if better reconstructions can be achieved with this network.

### Fully Convolutional Network
Convolutional Neural Networks (CNNs) are better equipped to handle images. Images lose spatial meaning when flattened out, using CNNs allows preserving the image's shape and can better infer spatial contexts.

Because of the flag `use_conv_autoencoder`, it's very easy to swap out the model. Make sure to set it to `True`.

First, let's define the model. Note how the `decoder` has the "inverse" of `Conv2d` as `ConvTranspose2d`. When stacking CNNs, the arguments `in_channels` and `out_channels` require special attention, here's a helpful [link](https://www.baeldung.com/cs/convolutional-layer-size#formula).
```
class AutoEncoderCNN(nn.Module):
    def __init__(self):
        super().__init__()

        # N, 1, 28, 28
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1),  # N, 16, 14, 14
            nn.ReLU(),
            # nn.BatchNorm2d(num_features=16),
            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1), # N, 32, 7, 7
            nn.ReLU(),
            # nn.BatchNorm2d(num_features=32),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=7),                      # N, 64, 1, 1
        )

        # N, 64, 1, 1
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=7),                                         # N, 32, 7, 7
            nn.ReLU(),
            # nn.BatchNorm2d(num_features=32),
            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2, padding=1, output_padding=1),  # N, 16, 14, 14
            nn.ReLU(),
            # nn.BatchNorm2d(num_features=16),
            nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=1),   # N, 1, 28, 28
            nn.Sigmoid()
        )

    def forward(self, x):
        x_enc = self.encoder(x)
        x_dec = self.decoder(x_enc)
        return x_dec
```
(Notice `nn.BatchNorm2d` layers have been commented out. Activate them and see how it makes a difference to the learning, if at all.)

Change `model` instantiation to:
```
if use_conv_autoencoder:
    print("Using CONV AutoEncoder")
    model = AutoEncoderCNN()
else:
    print("Using LINEAR AutoEncoder")
    model = AutoencoderLinear()
```
This will automatically select the right model depending on the flag `use_conv_autoencoder`. The loss function and optimizer stay the same.

Re-run the training loop. You should notice the loss to not only keep going down but also that the loss is lower for the corresponding epochs when we were training `AutoencoderLinear`.
![epoch_1](/images/autoencoders/CNN/AutoEncoderCNN_epoch_0.png)
![epoch_5](/images/autoencoders/CNN/AutoEncoderCNN_epoch_4.png)
![epoch_10](/images/autoencoders/CNN/AutoEncoderCNN_epoch_9.png)
![epoch_15](/images/autoencoders/CNN/AutoEncoderCNN_epoch_14.png)
![epoch_19](/images/autoencoders/CNN/AutoEncoderCNN_epoch_19.png)

### Comparison
CNNs clearly win, which comes as no surprise considering we're working with images after all. Let's compare the difference between fully connected network and fully convolutional network after 1 epoch and 20 epochs.

_After 1 epoch_

| AE type | Loss | Originals vs.<br>Reconstructions |
| :--------: | :--------: | :--------: |
| Fully connected | 0.048625 | ![epoch_1](/images/autoencoders/Linear/AutoEncoderLinear_epoch_0.png) |
| Fully convolutional | 0.010315 | ![epoch_1](/images/autoencoders/CNN/AutoEncoderCNN_epoch_0.png) |

_After 20 epochs_

| AE type | Loss | Originals vs.<br>Reconstructions |
| :--------: | :--------: | :--------: |
| Fully connected | 0.025885 | ![epoch_1](/images/autoencoders/Linear/AutoEncoderLinear_epoch_19.png) |
| Fully convolutional | 0.002479 | ![epoch_1](/images/autoencoders/CNN/AutoEncoderCNN_epoch_19.png) |

CNNs being clearly superior, with epoch 1 reconstructions having a lower training loss than fully-connected's epoch 20 loss, we'll move forward with the trained model of `AutoEncoderCNN`.

## Interpolating in latent space
Let's take two numbers, and interpolate between them .. _in MNIST's latent space_. We will visualize one number gradually "morph" into the other.

Set the model to eval mode:
```
model.eval()
```

Let's create two helper functions so that we can easily grab a randomly selected image from its target label. For example, for a target label `5`, we want to get an image of `5` randomly picked out of all the `5`'s in the dataset.
- `organize_dataset_labels()` needs to be called just once
- after `organize_dataset_labels()` has been called once, `get_img_of_digit()` can be used to get an image of whichever digit out of 0-9

```
def organize_dataset_labels():
    dataset_labels = [[] for _ in range(10)]

    for i, (_, label) in enumerate(dataset):
        dataset_labels[label].append(i)

    return dataset_labels

def get_img_of_digit(digit: int):
    img = dataset[random.choice(dataset_labels[digit])][0]
    return img
```

To verify both the above helper methods work as expected, run these two lines a bunch of times visualizing a different digit each time:
```
digit_to_visualize = 5
plt.imshow(get_img_of_digit(digit_to_visualize).squeeze())
```

Now let's set up the method that will interpolate between two digits and plot the results:
```
def interpolate_between_digits_and_plot(digit_1: int, digit_2: int):
    x1 = get_img_of_digit(digit_1)
    x2 = get_img_of_digit(digit_2)
    x = torch.stack([x1,x2])
    embedding = model.encoder(x)

    # embeddings of digit_1 and digit_2
    e1 = embedding[0]
    e2 = embedding[1]

    embedding_values = []
    for i in range(0, 10):
        e = e2 * (i/10) + e1 * (10-i)/10
        embedding_values.append(e)
    embedding_values = torch.stack(embedding_values)

    recons = model.decoder(embedding_values)

    plt.figure(figsize=(10, 2))
    for i, recon in enumerate(recons.detach().numpy()):
        plt.subplot(2,10,i+1)
        plt.imshow(recon[0])
    plt.subplot(2,10,11)
    plt.imshow(x1[0])
    plt.subplot(2,10,20)
    plt.imshow(x2[0])

    plt.show()
```
The inner workings of `interpolate_between_digits_and_plot()` should be fairly easy to follow:
1. For the input arguments `digit_1` and `digit_2`, get their images using `get_img_of_digit()`.
2. Get their embeddings by passing the images through the _encoder_ of the trained model.
3. Interpolate between the two embeddings `e1` and `e2`. Store the interpolated embeddings in `embedding_values`.
4. Pipe the interpolated embeddings through the `decoder` of the trained model, to obtain images from embeddings. These images show `digit_1` transforming into `digit_2`.
5. Rest of the method is just plotting.

### Example interpolations
```
interpolate_between_digits_and_plot(0, 8)
```
![interp_0_8_1](/images/autoencoders/interpolations/interp_0_8_1.png)
![interp_0_8_2](/images/autoencoders/interpolations/interp_0_8_2.png)

```
interpolate_between_digits_and_plot(5, 1)
```
![interp_5_1_1](/images/autoencoders/interpolations/interp_5_1_1.png)
![interp_5_1_2](/images/autoencoders/interpolations/interp_5_1_2.png)

```
interpolate_between_digits_and_plot(9, 6)
```
![interp_9_6_1](/images/autoencoders/interpolations/interp_9_6_1.png)
![interp_9_6_2](/images/autoencoders/interpolations/interp_9_6_2.png)


## References
The following links were used as source materials for this article:
- [Autoencoder In PyTorch - Theory & Implementation](https://youtu.be/zp8clK9yCro)
- [Transpose Convolutions and Autoencoders](https://www.cs.toronto.edu/~lczhang/360/lec/w05/autoencoder.html)
- [A Simple AutoEncoder and Latent Space Visualization with PyTorch](https://medium.com/@outerrencedl/a-simple-autoencoder-and-latent-space-visualization-with-pytorch-568e4cd2112a)

