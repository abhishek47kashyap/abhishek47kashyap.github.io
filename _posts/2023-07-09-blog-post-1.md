---
title: 'Neural Style Transfer: quick and easy'
date: 2023-07-09
permalink: /posts/2023/07/neural_style_transfer/
# tags:
#   - cool posts
#   - category1
#   - category2
---

Neural Style Transfer is a process that takes as input two images, and transfers the *style* of one image to the other image.

| Original Image<br>(input) | Style Image<br>(input) | Stylized Image<br>(output) |
| :--------: | :--------: | :--------: |
| <img src="/images/neural_style_transfer/german_shepherd.png" alt="original_image" width="300px"> | <img src="/images/neural_style_transfer/starry_night.png" alt="style_image" width="300px"> | ![stylized_image](/images/neural_style_transfer/generated_3000.png) |

The original paper published in 2015 can be found here: [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576).

This post shows how to quickly put together a neural style transfer setup. The code presented in this post is taken from [Pytorch Neural Style Transfer Tutorial](https://youtu.be/imX4kSKDY7s) with a few modifications of my own.

## Let's get going
Basic familiarity with the `Python` programming language and the framework `pytorch` is assumed.

Here's the entire code (less than 120 lines), with section-wise explanation down below:
```
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.models as models
from torchvision.utils import save_image
from PIL import Image


class VGG(nn.Module):
    def __init__(self):
        super().__init__()

        self.chosen_features = [
            '0',  # Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            '5',  # Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            '10',  # Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            '19',  # Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            '28',  # Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        ]

        self.model = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features[:29]
        print("Model initialized")

    def forward(self, x):
        features = []

        for layer_idx, layer in enumerate(self.model):
            x = layer(x)

            if str(layer_idx) in self.chosen_features:
                features.append(x)

        return features


def load_image(image_name):
    img = Image.open(image_name)
    img = loader(img).unsqueeze(0)
    return img.to(device)


if __name__ == "__main__":
    device = torch.device("gpu" if torch.cuda.is_available() else "cpu")
    model = VGG().to(device=device).eval()  # eval() to freeze pre-trained weights
    img_size = 256

    loader = transforms.Compose(
        [
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor()
        ]
    )

    original_image = load_image("german_shepherd.png")
    style_image = load_image("starry_night.png")
    print("Images loaded")
    generated_image = original_image.clone().requires_grad_(True)

    # hyper-parameters
    total_steps = 6000  # number of times generated image will be modified
    learning_rate = 0.001
    alpha = 1       # content loss
    beta = 0.01     # style loss
    optimizer = optim.Adam([generated_image], lr=learning_rate)

    print("Starting generation..")
    for step in range(total_steps):
        print(f"{step+1}/{total_steps}")
        generated_image_features = model(generated_image)
        original_image_features = model(original_image)
        style_image_features = model(style_image)

        style_loss = 0.0
        content_loss = 0.0

        for generated_img_feature, original_img_feature, style_img_feature in zip(
            generated_image_features, original_image_features, style_image_features
        ):
            batch_size, channel, height, width = generated_img_feature.shape    # will change for each conv layer block

            # content loss
            content_loss += torch.mean((generated_img_feature - original_img_feature) ** 2)

            # style loss
            g = generated_img_feature.view(channel, height*width)
            G = g.mm(g.t())
            a = style_img_feature.view(channel, height*width)
            A = a.mm(a.t())
            style_loss += torch.mean((G - A) ** 2)

        total_loss = (alpha * content_loss) + (beta * style_loss)
        print(f"\tTotal loss = {total_loss}")

        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

        # saving images
        if step % 200 == 0:
            generated_img_name = "generated_" + str(step) + ".png"
            save_image(generated_image, generated_img_name)

    print("EXIT")
```

Pretty simple, right? Let's rip it apart.

### Breakdown

We start off with the required imports:
```
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.models as models
from torchvision.utils import save_image
from PIL import Image
```

Next, we define the class `VGG` in which we initialize the VGG-19 model with its default weights. Check out the `pytorch` [documentation](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg19.html) for more information.
```
class VGG(nn.Module):
    def __init__(self):
        super().__init__()

        self.chosen_features = [
            '0',  # Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            '5',  # Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            '10',  # Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            '19',  # Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            '28',  # Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        ]

        self.model = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features[:29]
        print("Model initialized")

    def forward(self, x):
        features = []

        for layer_idx, layer in enumerate(self.model):
            x = layer(x)

            if str(layer_idx) in self.chosen_features:
                features.append(x)

        return features
```
So what's going on with `chosen_features`? Think of it as the *specific layers* in the neural network that we are really interested in. That is what those numbers `0`, `5`, .., `28` are about. For each of those layers, we want to extract the state of the original input subjected to "neural-network-magic" up until that specific layer. In other words, we've created observer windows at multiple points during forward pass of the network. Hopefully this helps make sense of `forward()` now?

Moving forward, we've a tiny helper function that uses `PIL` to load an image. The `unsqueeze()` is needed for some dimension adjustment of the image in a way that `pytorch` expects.
```
def load_image(image_name):
    img = Image.open(image_name)
    img = loader(img).unsqueeze(0)
    return img.to(device)
```

Right, now we delve into the `if __name__ == "__main__"` where we start using all of the stuff we set up above.

This is the standard way to indicate "use GPU if available, else go with CPU":
```
device = torch.device("gpu" if torch.cuda.is_available() else "cpu")
```

We then instantiate the `VGG` class and freeze the weights with `eval()`. We do not do any training of our own and simply stick with `VGG19_Weights.DEFAULT`.
```
model = VGG().to(device=device).eval()
```

### Check back again
Sorry, this post is not yet complete and I need slightly longer to finish writing this up.

If you can't wait to get started using this code with your own images, look for these lines in `if __name__ == "__main__"`:
```
    original_image = load_image("german_shepherd.png")
    style_image = load_image("starry_night.png")
```
Replace them with your own filepaths, and you should be good to go.

Feel free to contact me if you spot a bug somewhere. We're all learning along the way :-)
